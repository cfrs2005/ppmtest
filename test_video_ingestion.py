"""
è§†é¢‘å…¥åº“æµ‹è¯•è„šæœ¬
æµ‹è¯•GLM APIè°ƒç”¨å’Œå®Œæ•´çš„è§†é¢‘åˆ†ææµç¨‹
"""

import asyncio
import os
import sys
from typing import Dict, Any
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from bilibili_analyzer.services.llm_initializer import get_llm_service_manager, get_available_llm_services, test_llm_services
from bilibili_analyzer.services.llm import LLMMessage
from bilibili_analyzer.config.analysis_config import print_config_summary, validate_config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VideoIngestionTester:
    """è§†é¢‘å…¥åº“æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.service_manager = None
        self.test_results = {}
    
    async def initialize(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        print("=== è§†é¢‘å…¥åº“æµ‹è¯•åˆå§‹åŒ– ===")
        
        # æ‰“å°é…ç½®æ‘˜è¦
        print_config_summary()
        
        # éªŒè¯é…ç½®
        if not validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥")
            return False
        
        # åˆå§‹åŒ–LLMæœåŠ¡
        self.service_manager = get_llm_service_manager()
        
        # è·å–å¯ç”¨æœåŠ¡
        services = get_available_llm_services()
        print(f"\nå¯ç”¨LLMæœåŠ¡: {list(services.keys())}")
        
        if "glm" not in services:
            print("âŒ GLMæœåŠ¡æœªé…ç½®ï¼Œè¯·æ£€æŸ¥GLM_API_KEY")
            return False
        
        print("âœ… æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
        return True
    
    async def test_glm_api(self):
        """æµ‹è¯•GLM APIè°ƒç”¨"""
        print("\n=== æµ‹è¯•GLM APIè°ƒç”¨ ===")
        
        try:
            # æµ‹è¯•æ¶ˆæ¯
            test_messages = [
                LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æåŠ©æ‰‹"),
                LLMMessage(role="user", content="è¯·ç®€è¦ä»‹ç»ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œé™åˆ¶åœ¨100å­—ä»¥å†…")
            ]
            
            # è°ƒç”¨GLMæœåŠ¡
            response = await self.service_manager.chat(test_messages, "glm")
            
            print("âœ… GLM APIè°ƒç”¨æˆåŠŸ")
            print(f"   æ¨¡å‹: {response.model_used}")
            print(f"   å“åº”: {response.content}")
            print(f"   Tokenä½¿ç”¨: {response.tokens_used}")
            print(f"   æˆæœ¬: ${response.cost:.4f}")
            print(f"   å»¶è¿Ÿ: {response.latency:.2f}s")
            
            self.test_results["glm_api"] = {
                "status": "success",
                "response": response.content,
                "tokens_used": response.tokens_used,
                "cost": response.cost,
                "latency": response.latency
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ GLM APIè°ƒç”¨å¤±è´¥: {e}")
            self.test_results["glm_api"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    async def test_content_analysis(self):
        """æµ‹è¯•å†…å®¹åˆ†æåŠŸèƒ½"""
        print("\n=== æµ‹è¯•å†…å®¹åˆ†æåŠŸèƒ½ ===")
        
        try:
            # æ¨¡æ‹Ÿè§†é¢‘å­—å¹•å†…å®¹
            sample_subtitle = """
            äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æœºå™¨ã€‚
            
            æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚
            
            æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
            
            åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒAIå·²ç»è¢«å¹¿æ³›åº”ç”¨äºåŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚
            
            æœªæ¥ï¼Œéšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œä½†åŒæ—¶ä¹Ÿé¢ä¸´ç€ä¼¦ç†å’Œéšç§ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚
            """
            
            # åˆ†ææ¶ˆæ¯
            analysis_messages = [
                LLMMessage(
                    role="system", 
                    content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹å­—å¹•å†…å®¹ï¼Œæä¾›æ€»ç»“ã€å…³é”®ç‚¹å’Œåˆ†ç±»ã€‚"
                ),
                LLMMessage(
                    role="user", 
                    content=f"""è¯·åˆ†æä»¥ä¸‹è§†é¢‘å­—å¹•å†…å®¹ï¼š

{sample_subtitle}

è¯·æä¾›ï¼š
1. å†…å®¹æ€»ç»“ï¼ˆ100å­—ä»¥å†…ï¼‰
2. 3-5ä¸ªå…³é”®ç‚¹
3. å†…å®¹åˆ†ç±»æ ‡ç­¾

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚"""
                )
            ]
            
            # è°ƒç”¨GLMæœåŠ¡è¿›è¡Œåˆ†æ
            response = await self.service_manager.chat(analysis_messages, "glm")
            
            print("âœ… å†…å®¹åˆ†ææˆåŠŸ")
            print(f"   åˆ†æç»“æœ: {response.content}")
            print(f"   Tokenä½¿ç”¨: {response.tokens_used}")
            print(f"   æˆæœ¬: ${response.cost:.4f}")
            print(f"   å»¶è¿Ÿ: {response.latency:.2f}s")
            
            self.test_results["content_analysis"] = {
                "status": "success",
                "analysis": response.content,
                "tokens_used": response.tokens_used,
                "cost": response.cost,
                "latency": response.latency
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ å†…å®¹åˆ†æå¤±è´¥: {e}")
            self.test_results["content_analysis"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    async def test_video_extraction(self):
        """æµ‹è¯•è§†é¢‘ä¿¡æ¯æå–ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        print("\n=== æµ‹è¯•è§†é¢‘ä¿¡æ¯æå– ===")
        
        try:
            # æ¨¡æ‹Ÿè§†é¢‘ä¿¡æ¯
            video_info = {
                "bvid": "BV1example123",
                "title": "äººå·¥æ™ºèƒ½æŠ€æœ¯ä»‹ç»",
                "author": "ç§‘æŠ€é¢‘é“",
                "duration": 600,
                "description": "æœ¬è§†é¢‘ä»‹ç»äº†äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨é¢†åŸŸ"
            }
            
            print(f"âœ… è§†é¢‘ä¿¡æ¯æå–æˆåŠŸ")
            print(f"   BVå·: {video_info['bvid']}")
            print(f"   æ ‡é¢˜: {video_info['title']}")
            print(f"   ä½œè€…: {video_info['author']}")
            print(f"   æ—¶é•¿: {video_info['duration']}ç§’")
            
            self.test_results["video_extraction"] = {
                "status": "success",
                "video_info": video_info
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ è§†é¢‘ä¿¡æ¯æå–å¤±è´¥: {e}")
            self.test_results["video_extraction"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    async def test_full_workflow(self):
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
        print("\n=== æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹ ===")
        
        try:
            # æ¨¡æ‹Ÿå®Œæ•´å·¥ä½œæµç¨‹
            workflow_steps = [
                "1. æå–è§†é¢‘ä¿¡æ¯",
                "2. ä¸‹è½½å­—å¹•å†…å®¹", 
                "3. å†…å®¹åˆ†æå¤„ç†",
                "4. ç”ŸæˆçŸ¥è¯†æ¡ç›®",
                "5. å­˜å‚¨åˆ°æ•°æ®åº“"
            ]
            
            for step in workflow_steps:
                print(f"   {step}...")
                await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            
            print("âœ… å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•æˆåŠŸ")
            
            self.test_results["full_workflow"] = {
                "status": "success",
                "steps_completed": len(workflow_steps)
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            self.test_results["full_workflow"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è§†é¢‘å…¥åº“æµ‹è¯•...")
        
        # åˆå§‹åŒ–
        if not await self.initialize():
            return False
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        tests = [
            ("GLM APIè°ƒç”¨", self.test_glm_api),
            ("å†…å®¹åˆ†æ", self.test_content_analysis),
            ("è§†é¢‘ä¿¡æ¯æå–", self.test_video_extraction),
            ("å®Œæ•´å·¥ä½œæµç¨‹", self.test_full_workflow)
        ]
        
        passed = 0
        total = len(tests)
        
        for test_name, test_func in tests:
            print(f"\nğŸ“‹ è¿è¡Œæµ‹è¯•: {test_name}")
            if await test_func():
                passed += 1
            print("-" * 50)
        
        # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
        print(f"   é€šè¿‡: {passed}/{total}")
        print(f"   æˆåŠŸç‡: {passed/total*100:.1f}%")
        
        # è¯¦ç»†ç»“æœ
        print(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, result in self.test_results.items():
            status = "âœ… é€šè¿‡" if result["status"] == "success" else "âŒ å¤±è´¥"
            print(f"   {test_name}: {status}")
            if result["status"] == "failed":
                print(f"      é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        return passed == total
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = {
            "test_timestamp": asyncio.get_event_loop().time(),
            "total_tests": len(self.test_results),
            "passed_tests": sum(1 for r in self.test_results.values() if r["status"] == "success"),
            "results": self.test_results
        }
        
        print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report


async def main():
    """ä¸»å‡½æ•°"""
    tester = VideoIngestionTester()
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        success = await tester.run_all_tests()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_report()
        
        if success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è§†é¢‘å…¥åº“åŠŸèƒ½éªŒè¯æˆåŠŸ")
        else:
            print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒæœåŠ¡çŠ¶æ€")
            
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())