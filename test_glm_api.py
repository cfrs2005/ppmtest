"""
ç®€åŒ–çš„GLM APIæµ‹è¯•è„šæœ¬
ç›´æ¥æµ‹è¯•GLM APIè°ƒç”¨åŠŸèƒ½
"""

import asyncio
import os
import sys
import json
from typing import Dict, Any
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleGLMTester:
    """ç®€åŒ–çš„GLMæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.api_key = os.getenv("GLM_API_KEY")
        self.api_base = os.getenv("GLM_API_BASE", "https://open.bigmodel.cn/api/paas/v4")
        self.model = os.getenv("GLM_MODEL", "glm-4-flash")
    
    async def test_glm_api(self):
        """æµ‹è¯•GLM APIè°ƒç”¨"""
        print("=== GLM APIæµ‹è¯• ===")
        
        # æ£€æŸ¥é…ç½®
        if not self.api_key:
            print("âŒ GLM_API_KEYæœªé…ç½®")
            return False
        
        print(f"âœ… é…ç½®æ£€æŸ¥é€šè¿‡")
        print(f"   API Base: {self.api_base}")
        print(f"   Model: {self.model}")
        
        try:
            # å°è¯•å¯¼å…¥openai
            import openai
            print("âœ… OpenAIåº“å¯¼å…¥æˆåŠŸ")
            
            # åˆ›å»ºå®¢æˆ·ç«¯
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            print("âœ… OpenAIå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•åŒæ­¥è°ƒç”¨
            print("ğŸ”„ æµ‹è¯•åŒæ­¥APIè°ƒç”¨...")
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹"},
                    {"role": "user", "content": "è¯·å›å¤'GLM APIæµ‹è¯•æˆåŠŸ'æ¥éªŒè¯æœåŠ¡æ­£å¸¸å·¥ä½œ"}
                ],
                max_tokens=100,
                temperature=0.7
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            print("âœ… GLM APIè°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”: {content}")
            print(f"   Tokenä½¿ç”¨: {tokens_used}")
            
            return True
            
        except ImportError:
            print("âŒ OpenAIåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")
            return False
        except Exception as e:
            print(f"âŒ GLM APIè°ƒç”¨å¤±è´¥: {e}")
            return False
    
    async def test_async_glm_api(self):
        """æµ‹è¯•å¼‚æ­¥GLM APIè°ƒç”¨"""
        print("\n=== å¼‚æ­¥GLM APIæµ‹è¯• ===")
        
        try:
            import openai
            print("âœ… OpenAIåº“å¯¼å…¥æˆåŠŸ")
            
            # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
            client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            print("âœ… å¼‚æ­¥OpenAIå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•å¼‚æ­¥è°ƒç”¨
            print("ğŸ”„ æµ‹è¯•å¼‚æ­¥APIè°ƒç”¨...")
            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æåŠ©æ‰‹"},
                    {"role": "user", "content": "è¯·ç®€è¦ä»‹ç»ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Œé™åˆ¶åœ¨50å­—ä»¥å†…"}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            print("âœ… å¼‚æ­¥GLM APIè°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”: {content}")
            print(f"   Tokenä½¿ç”¨: {tokens_used}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å¼‚æ­¥GLM APIè°ƒç”¨å¤±è´¥: {e}")
            return False
    
    async def test_content_analysis(self):
        """æµ‹è¯•å†…å®¹åˆ†æåŠŸèƒ½"""
        print("\n=== å†…å®¹åˆ†ææµ‹è¯• ===")
        
        try:
            import openai
            client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            
            # æ¨¡æ‹Ÿè§†é¢‘å­—å¹•å†…å®¹
            sample_content = """
            äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æœºå™¨ã€‚
            æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚
            æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
            """
            
            # åˆ†ææ¶ˆæ¯
            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹å†…å®¹å¹¶æä¾›æ€»ç»“å’Œå…³é”®ç‚¹ã€‚"
                    },
                    {
                        "role": "user",
                        "content": f"""è¯·åˆ†æä»¥ä¸‹è§†é¢‘å­—å¹•å†…å®¹ï¼š

{sample_content}

è¯·æä¾›ï¼š
1. å†…å®¹æ€»ç»“ï¼ˆ50å­—ä»¥å†…ï¼‰
2. 3ä¸ªå…³é”®ç‚¹

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚"""
                    }
                ],
                max_tokens=500,
                temperature=0.7
            )
            
            content = response.choices[0].message.content
            tokens_used = response.usage.total_tokens
            
            print("âœ… å†…å®¹åˆ†ææˆåŠŸ")
            print(f"   åˆ†æç»“æœ: {content}")
            print(f"   Tokenä½¿ç”¨: {tokens_used}")
            
            return True
            
        except Exception as e:
            print(f"âŒ å†…å®¹åˆ†æå¤±è´¥: {e}")
            return False
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹GLM APIæµ‹è¯•...")
        
        tests = [
            ("åŒæ­¥APIè°ƒç”¨", self.test_glm_api),
            ("å¼‚æ­¥APIè°ƒç”¨", self.test_async_glm_api),
            ("å†…å®¹åˆ†æ", self.test_content_analysis)
        ]
        
        passed = 0
        total = len(tests)
        
        for test_name, test_func in tests:
            print(f"\nğŸ“‹ è¿è¡Œæµ‹è¯•: {test_name}")
            if await test_func():
                passed += 1
            print("-" * 50)
        
        # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
        print(f"   é€šè¿‡: {passed}/{total}")
        print(f"   æˆåŠŸç‡: {passed/total*100:.1f}%")
        
        if passed == total:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GLM APIé›†æˆæˆåŠŸ")
            print("âœ… è§†é¢‘å…¥åº“åŠŸèƒ½éªŒè¯å®Œæˆ")
        else:
            print(f"\nâš ï¸  {total-passed}ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒæœåŠ¡çŠ¶æ€")
        
        return passed == total


async def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("âœ… ç¯å¢ƒå˜é‡åŠ è½½æˆåŠŸ")
    except ImportError:
        print("âš ï¸  python-dotenvæœªå®‰è£…ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")
    
    # è¿è¡Œæµ‹è¯•
    tester = SimpleGLMTester()
    
    try:
        await tester.run_all_tests()
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())